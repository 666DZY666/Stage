一份超全的PyTorch资源列表

    https://zhuanlan.zhihu.com/p/47318546


1、LeNet5
#pytorch 实现lenet5

#导入gzip，struct类
import gzip,struct
#从numpy模块导入np类
import numpy as np


import torch
#import torch.nn as nn
#import torch.nn.functional as F
#模块参数的子类
from torch import nn
from torch.nn import functional as F
#对任意标量进行求导
from torch.autograd import Variable

#表示dataset的抽象类
from torch.utils.data import  tensorDataset,DataLoader
#模型结构加载数据和图片转换工具
from torchvision import transforms

import math

#读取数据的函数，先读取标签，再读取图片
def _read(image,label):
    mnist_dir = 'data/'
    with gzip.open(mnist_dir+label) as flbl :
        magic, num = struct.unpack(">II",flbl.read(8))
        label = np.fromstring(flbl.read(),dtyper = np.int8)
    with gzip.open(mnist_dir+image,'rb')as fimg:
        magic,num,rows,cols = struct.unpack(">III",fimg.read(16))
        image = np.fromstring(fimg.read(),dtype=np.uint8).reshape(len(label),rows,cols)
        return image,label
#读取数据
def get_data():
    train_img,train_label= _read(
            'train_image-idex3-ubyte.gz',
        'train_image-idex1-ubyte.gz')
    test_img, test_label = _read(
        't10k-images-idx3-ubyte.gz',
        't10k-label-idx1-ubyte.gz')
    return [train_img,train_label,test_img,test_label]

#定义lenet5模型
class LeNet5(nn.module):
    def __int__(self):
        #定于构造函数，定义网络结构
        super().__int__()
        #定义卷积层，1个输入通道，6个输出通道，5*5的卷积filter，外围的两圈填充全为0，因为卷积的输入为32*32
        self.conv1 = nn.Con2d(1,6,5,padding = 2)
        #第二个卷积层，6个输入，16个输出，5*5的卷积filter
        self.conv2 = nn.Con2d(6,16,5)

        #后面定义三个全链接层
        self.fc1 = nn.Linear(16*5*5,120)
        self.fc2 = nn.Linear(120,84)

        self.fc3 = nn.Linear(84,10)

    def forward(self,x):
        #定义反向传播函数
        #先卷积，然后调用relu激活函数，再最大池化操作
        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))
        #第二次卷积+池化操作
        x = F.max_pool2d(F.relu(self.conv2(x)),(2,2,))
        #重新塑形，将多维数据重新塑造成为二维数据，256*400
        x = x.view(-1,self.num_flat_features(x))
        print('size',x.size())
        #第一个全链接
        x=F.relu(self.fc1(x))
        x=F.relu(self.fc2(x))
        x= self.fc3(x)
        return  x

    def num_flat_features(self,x):
        #x.size()返回值为（256，16，5，5）size的值为（16，5，5），256是batch_size
        #x.size返回的一个元祖，size表示截取元组中第二个开始的数字
        size = x.size()[1:]
        num_feature = 1
        for s in size:
            num_feature *= s
            return  num_feature
    #定义一个超参数
use_gpu = torch.cuda.is_available()
batcbatch_size = 256
#DataLoad的参数
kwargs = {'num_workers':2,'pin_mempry':True}

def weight_init(m):
    if isinstance(m,nn.Conv2d):
        n = m.kernel_size[0]*m.kernel_size[1]*m.out_channels
        m.weight.data.normal_(0,math.sqrt(2./n))
    elif isinstance(m,nn.BatchNorm2d):
        m.weight.data.fill_(1)
        m.bias.data.zero_()

#训练数据
def train(epoch):
    #调用前向传播
    model.train()
    for batch_idx,(data,target) in enumerate(train_loader):
        if use_gpu:
            data,target = data.cuda(),target.cuda()
        #定义Variable类型，能自动求微分
        data,target = Variable(data),Variable(target)
        #初始化时要清空梯度
        optimimzer.zero_grad()
        output = model(data)
        loss = criterion(output,target)
        loss.backward()
        #更新权重
        optimimzer.step()
        if batch_idx % 100 ==0:

            print('Train Epoch:{} [{}/{}({:0f}%)]\tLoss:{:.6f}'.format(
                epoch,batch_idx*len(data),len(train_loader.dataset),
                100.*batch_idx/len(train_loader),loss.data[0]))
def test():
    module.eval()
    test_loss = 0
    correct = 0
    for data ,target in test_loader:
        if use_gpu:
            data,target = data.cuda(),target.cuda()
        data,target = Variable(data,volatile = True),Variable(target)
        output = module(data)

        test_loss += criterion(output,target).data[0]
        #获取得分最高的类别
        pred = output.data.max(1,keepdim = True)[1]
        correct  += pred.eq(target.data.view_as(pred)).cpu().sum()

    test_loss /=len(test_loader.dataset)
    print('\nTest set:Average loss:{：4f},Accuracy: {}/{} ({:,2f}%)\n'.format(
        test_loss,correct,len(test_loader.dataset),
        100.*correct / len(test_loader.dataset)))
#获取数据
X,y,Xt,yt = get_data()

train_x,train_y = torch.from_numpy(X.reshape(-1,1,28,28)).float(),torch.from_numpy(y.astype(int))
test_x,test_y = [torch.from_numpy(Xt.reshape(-1,1,28,28)).float(),torch.from_numpy(yt.astype(int))]

#打包数据和标签
train_dataset = tensorDataset(data_tensor =train_x,target_tensor = train_y)
test_dataset = tensorDataset(data_tensor=test_x,target_tensor = test_y)

#定义数据加载器
train_loader = DataLoader(dataset = train_dataset,shuffle = True,batch_size = batch_size,**kwargs)
test_loader = DataLoader(dataset = test_dataset,shuffle = True,batch_size = batch_size,**kwargs)

#实例化网络
model = LeNet5()
if use_gpu:
    module = module.cuda()
    print('USE GPU')
else:
    print('USE GPU')

#定义代价函数，使用交叉熵验证
criterion = nn.CrossEntropyLoss(size_average =False)
#直接定义优化器，而不是调用backward
optimimzer = torch.optim.Adam(model.parmeters(),lr = 0.001,betas=(0.9,0.99))

#调用函数执行训练和测试
model.apply(weight_init)

for epoch in range(1,501):
    print('------------start train----------')
    train(epoch)
    print('-----------end train-----------')

    print('----------start test----------')
    test()
    print('----------end test-----------')

